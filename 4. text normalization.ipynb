{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Text Normalization?\n",
    "Text normalization is a key step in NLP that cleans and preprocesses data into a usable, standard and “less-random” format. <br><br>\n",
    "\n",
    "Text normalization involves various techniques such as lowercasing, removing special characters and stop words removal etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need text normalization?\n",
    "Here are two main reasons why we need text normalization: <br><br>\n",
    "\n",
    "1. Reduces complexity:<br><br>\n",
    "Human language is full of complexities such as slangs, abbreviations and different grammatical forms of the same word.<br>\n",
    "Text normalizations helps reduce these complexities by transforming the text into a standard and consistent format.<br>\n",
    "\n",
    "2. Improves Efficiency:<br><br>\n",
    "By reducing the number of unique forms that a word can take, text normalization improves the efficiency of NLP models.\n",
    "For instance, a model doesn’t need to learn the difference between “play” and “playing” if it understands they both convey the same core meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques of text normalization:\n",
    "Following are some of the main techniques used for text normalization:\n",
    "\n",
    "#### 1. Lowercasing:\n",
    "Lowercasing is a technique that transforms all text into lowercase to ensure standard formats for all characters.<br><br>\n",
    "\n",
    "Here’s a simple function that implements lowercasing with python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "  \"\"\"\n",
    "  This function takes text and returns the text in lowercase\n",
    "  \"\"\"\n",
    "\n",
    "  return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a sample text to demonstrate lowercasing a piece of text'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a Sample Text to demonstrate lowercasing a piece of TEXT\"\n",
    "\n",
    "lowercase_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Removing punctuation:\n",
    "There are cases where we need to get rid of punctuations. <br><br>\n",
    "\n",
    "For example, if your word embeddings matrix doesn’t support special characters, we need to get rid of them.<br><br>\n",
    "\n",
    "Here’s a short function that implements punctuation removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello How are you doing today I hope everything is going well Dont forget to bring your umbrella when it rains and make sure to smile'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "def remove_punctuations(text,punctuations):\n",
    "    for punctuation in punctuations:\n",
    "        if punctuation in text:\n",
    "            text = text.replace(punctuation, '')\n",
    "    return text.strip()\n",
    "\n",
    "text = \"Hello! How are you doing today? I hope everything is going well. Don't forget to bring your umbrella when it rains, and make sure to smile! :)\"\n",
    "\n",
    "text_without_punct = remove_punctuations(text, punctuations)\n",
    "text_without_punct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming & lemmatization:\n",
    "Stemming and lemmatization are techniques that reduce a word to its base form. <br><br>\n",
    "\n",
    "For example, “playing”, “played”, “plays” are all reduced to “play” and hence, converting all these forms to a standard format.<br><br>\n",
    "\n",
    "Here’s a python code that implements stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The :  the\n",
      "quick :  quick\n",
      "brown :  brown\n",
      "foxes :  fox\n",
      "are :  are\n",
      "jumping :  jump\n",
      "over :  over\n",
      "the :  the\n",
      "lazy :  lazi\n",
      "dogs :  dog\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "sentence = \"The quick brown foxes are jumping over the lazy dogs\"\n",
    "words = word_tokenize(sentence)\n",
    " \n",
    "for word in words:\n",
    "    print(word, \": \", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a python code that implements lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The: the\n",
      "quick: quick\n",
      "brown: brown\n",
      "foxes: fox\n",
      "are: be\n",
      "jumping: jump\n",
      "over: over\n",
      "the: the\n",
      "lazy: lazy\n",
      "dogs: dog\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
    "\n",
    "text = nlp(text)\n",
    "\n",
    "lemmatized_tokens = [token.lemma_ for token in text]\n",
    "\n",
    "for original, lemmatized in zip(text,lemmatized_tokens):\n",
    "    print(str(original) + \": \" + lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stop words Removal:\n",
    "For a variety of NLP tasks, words like “are”, “the”, “an” or “on” do not carry any useful information.<br><br>\n",
    "\n",
    "Hence, we remove these stop words for efficiency and reducing complexity.<br><br>\n",
    "\n",
    "Here’s a sample python function that accomplishes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello today hope everything going well Dont forget bring umbrella rains make sure smile'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "\n",
    "    filtered_sentence = [word for word in word_tokens if not word.lower() in stop_words]\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "remove_stopwords(text_without_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Expanding contractions:\n",
    "Contractions are words like “I’m”, “We’re” or “doesn’t”.<br><br>\n",
    "\n",
    "These are basically a short way of writing “I am”, “We are” and “Does not” respectively.<br><br>\n",
    "\n",
    "There are two main reasons why we should expand such contractions:<br><br>\n",
    "\n",
    "1. Computer doesn’t understand that “I’m” and “I am” mean the same thing.<br>\n",
    "2. It increases dimensionality of document-term matrix as we have to have separate columns for “I’m” and “I am”.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a python function that expands contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    " \n",
    "def expand_contractions(text):\n",
    "  expanded_text = []\n",
    "  \n",
    "  for word in text.split():\n",
    "    expanded_text.append(contractions.fix(word))  \n",
    "     \n",
    "  expanded_text = ' '.join(expanded_text)\n",
    "  return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot believe it is already Friday! It is been a long week, has not it? I am looking forward to the weekend.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I can't believe it's already Friday! It's been a long week, hasn't it? I'm looking forward to the weekend.\"\n",
    "\n",
    "expand_contractions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
